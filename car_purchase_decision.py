# -*- coding: utf-8 -*-
"""Car_Purchase_Decision.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16wPGmgeOSMxapi5Z2ToCv8Vtk2N1qtSM

# Import All Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import sklearn.metrics  as met
# %matplotlib inline
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV as grid
import warnings

warnings.filterwarnings('always')

"""Read dataset from csv using pandas library"""

data = pd.read_csv('/content/car_data.csv')
data.head()

"""#Exploratory Data Analysis

Print rows and columns in dataset to know how much data and features we have
"""

print("There are {} rows in dataset".format(data.shape[0]))
print("There are {} columns in dataset".format(data.shape[1]))

data.info()

data.describe()

"""Check if there is missing value in dataset"""

data.isnull().sum()

"""We see that there is no missing value in the dataset. 


Therefore we can skip to the next steps

#One Hot Encoding

we see that gender is a categorical data, therefore we need to change it into numerical.
"""

encoding = {'Gender': {'Male':0, 'Female':1}}
data.replace(encoding, inplace=True)

data.head()

"""#Find the correlation between all features"""

plt.figure(figsize=(10,8))
correlation_matrix = data.corr().round(2)

#Print scores using heatmap 
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix for All Features ", size=20)

"""Because features 'User ID' and 'Gender' have the lowest correlation, then we need to remove it"""

data.drop(['User ID'], axis=1, inplace=True)
data.drop(['Gender'], axis=1, inplace=True)
data.head()

"""#Univariate Analysis

We analyze 'Purchased' column because we want to know how much 1 (purchased) and 0 (not purchased) in dataset
"""

data['Purchased'].value_counts()

"""Analyze maximum age and minimum age in dataset, and maximum anual salary and minimum anual salary in dataset"""

age = data['Age']
annual_salary = data['AnnualSalary']
print('maximum age in dataset is {} and minimum age in dataset is {}' .format(age.max(), age.min()))
print('maximum annual salary in dataset is {} and minimum annual salary in dataset is {}' .format(annual_salary.max(), annual_salary.min()))

"""# Multivariate Analysis

In this step, we will analyze all **features**.

We remove 'Purchased' because that is target column and not features
"""

features = data.drop(['Purchased'], axis=1)
features.head()
features.hist(bins=30, figsize=(15,5))
plt.show()

"""# Handle Outliers

To find the outliers, first we need to plot all the features to find the outliers, we using boxplot to visualize them
"""

fig = plt.figure(figsize= (15,5))
columns = ['Age', 'AnnualSalary']
for index in range(1,3):
  ax = fig.add_subplot(1,2, index)
  ax.set_title("plot {}".format(columns[index-1]))
  sns.boxplot(x=data[columns[index-1]], ax=ax)

plt.show()

"""Because we do not have any outliers, therefore we can move to next steps

#Data Preparation

In this data preparation, we split train and test data
"""

x = data[['Age', 'AnnualSalary']].values
y = data['Purchased'].values

"""Standarization to make value in features become 0 to 1"""

scaler = StandardScaler().fit(x)
x = scaler.transform(x)

x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=10, test_size= 0.2)

print("x_train: {}".format(x_train.shape))
print("y_train: {}".format(y_train.shape))
print("x_test: {}".format(x_test.shape))
print("y_test: {}".format(y_test.shape))

"""# Random Fores Classifier

We use Random Fores Classifier algorithm to classify which customer will purchase the car 
"""

model = RandomForestClassifier()
parameters = {
    'n_estimators' : [100, 200, 300, 400, 500],
    'criterion': ['gini', 'entropy'],
}

#melatih model dengan fungsi fit
grid_search = grid(model, parameters)
grid_search.fit(x_train,y_train)
#menampilkan parameter terbaik dari objek grid_search
print(grid_search.best_params_)

model= RandomForestClassifier(n_estimators = 400, criterion= 'gini')
model.fit(x_train, y_train)
# predictions = model.predict(X_test)
# accuracy_score(y_test, predictions)

predict = model.predict(x_test)
print("Logistic Regression Mean Squared Error: {}".format(mean_squared_error(y_pred = predict, y_true=y_test)))
print("Logistic Regression Accuracy: {}".format(accuracy_score(y_test, predict)))
print("Confussion Matrix: \n{}".format(met.confusion_matrix(y_test, predict)))
print("Classification Report: \n{}".format(classification_report(y_test, predict)))